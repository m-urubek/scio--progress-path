[
  {
    "round": 1,
    "questions": [
      {
        "question": "How should the AI determine if a student 'demonstrated understanding' for binary goals? What criteria or examples define 'understanding' vs 'not understanding' (REQ-AI-003)?",
        "reason": "Binary goal completion is subjective. The AI needs clear guidance on what constitutes 'understanding' - is a correct answer enough? Must the student show their work? This directly affects when goals are marked complete."
      },
      {
        "question": "What happens when the AI cannot reliably classify whether a student message is on-topic or off-topic (edge cases like tangential questions, asking for clarification about unrelated topics, etc.)?",
        "reason": "The warning/escalation system depends on accurate off-topic detection. Unclear classification criteria could lead to false positives (unfair warnings) or false negatives (missing struggling students)."
      },
      {
        "question": "Should students be able to see their completed sub-tasks for percentage goals (e.g., 'Equations solved: 1, 2' not just '66%'), or only the aggregate percentage?",
        "reason": "REQ-GOAL-005 says students see 'current progress indicator' but doesn't specify granularity. Showing which steps are done could help students track their work, especially for goals with many steps."
      },
      {
        "question": "What is the expected behavior when a teacher confirms a goal interpretation but then discovers it was wrong during a live session? Can they create a new group with the same name, or is there a workaround?",
        "reason": "Groups are immutable and permanent (REQ-GROUP-016, Out of Scope #4). If the AI misinterprets a goal and the teacher only realizes during the session, there's no recovery path described."
      },
      {
        "question": "What should happen if the LLM API becomes unavailable mid-session (after students have joined)? Should chat continue without AI guidance, or should the session pause?",
        "reason": "REQ-GROUP-009 covers API failure during goal creation, but not during active chat sessions. This affects whether students can continue working if the AI goes down temporarily."
      },
      {
        "question": "For the 'activity resumption notification' (REQ-AI-012), what notification mechanism should be used - a toast, an alert in the same alert queue, or something else?",
        "reason": "The requirement says 'notify the teacher that the student has resumed activity' but doesn't specify if this is a dismissible alert, a transient notification, or another UI element."
      },
      {
        "question": "What is the expected scale for concurrent groups? Should one teacher be able to run multiple groups simultaneously, and if so, should there be cross-group visibility on the dashboard?",
        "reason": "The PRD mentions 30 concurrent students per group (PT-003) but doesn't address whether a teacher might monitor multiple active groups at once, which affects dashboard design."
      },
      {
        "question": "How should the system handle browser tab/window close or device going to sleep? Should the inactivity timer only count when the session is actively connected, or also when disconnected?",
        "reason": "REQ-AI-009 mentions 'inactivity based on a fixed 10-minute timeout from the last message sent' but a student might close their laptop, which breaks the SignalR connection. Does inactivity count during disconnection?"
      },
      {
        "question": "For the QR code join URL, should it be a short URL or contain the full join path? What domain/base URL should be used for the QR code content?",
        "reason": "REQ-GROUP-002 says a QR code is generated but doesn't specify the URL structure. This affects both UX (scannable size) and deployment configuration."
      },
      {
        "question": "What is the maximum expected length for goal descriptions that the AI must parse? Are there any constraints on goal complexity (e.g., max number of steps for percentage goals)?",
        "reason": "The PRD shows simple examples like 'solve 3 equations' but doesn't address complex goals with many steps, which affects AI parsing complexity and progress bar granularity."
      }
    ],
    "answers": [
      {
        "question": "How should the AI determine if a student 'demonstrated understanding' for binary goals? What criteria or examples define 'understanding' vs 'not understanding' (REQ-AI-003)?",
        "answer": "The PRD provides clear guidance via REQ-AI-004: \"err on the side of giving credit—if a student demonstrates partial understanding, award the progress.\"\n\nFor binary goals, the AI should mark as \"understood\" when the student:\n1. Provides a correct answer or explanation (even if not perfectly worded)\n2. Shows correct reasoning/logic, even with minor errors in execution\n3. Demonstrates any partial understanding of the concept\n\nThe AI should NOT require:\n- Perfect phrasing or formal language\n- Showing all work/steps (not specified in PRD)\n- Multiple demonstrations of the same concept\n\nImplementation approach (keep it simple):\n- The AI prompt should instruct: \"Award the binary goal as complete if the student shows they understand the core concept. Partial or imperfect explanations that demonstrate comprehension count as understanding. When in doubt, give credit.\"\n- No need for complex rubrics or multi-factor scoring - a single LLM judgment call with \"bias toward credit\" is sufficient\n- The teacher already confirmed the goal interpretation (REQ-GROUP-005), so the AI knows what \"understanding\" means for that specific goal\n\nExample binary goal: \"explains the difference between linear and quadratic equations\"\n- COMPLETE: \"linear is straight line, quadratic is curved/parabola\" (correct concept, informal)\n- COMPLETE: \"quadratic has x² term\" (partial but shows understanding)\n- NOT COMPLETE: \"I don't know\" or completely wrong answer"
      },
      {
        "question": "What happens when the AI cannot reliably classify whether a student message is on-topic or off-topic (edge cases like tangential questions, asking for clarification about unrelated topics, etc.)?",
        "answer": "The PRD already establishes a clear \"when in doubt, classify as on-topic\" principle. No special edge-case handling is needed beyond that.\n\n**The PRD's existing rules resolve ambiguity:**\n\n1. **REQ-CHAT-009 / REQ-AI-008:** \"If a message contains *any* substantive goal-relevant content (even mixed with off-topic content), it shall be classified as on-topic.\" This is the dominant rule — any hint of relevance means on-topic.\n\n2. **REQ-AI-004:** \"Err on the side of giving credit.\" While this directly addresses progress, the generous-by-default philosophy applies to the whole AI posture.\n\n3. **REQ-AI-007:** The warning count resets when the student returns on-topic. This means a single false-positive off-topic classification is self-correcting — the student's next on-topic message resets the counter and no teacher alert fires.\n\n**Implementation guidance for the LLM prompt:**\n\nThe AI should classify a message as off-topic ONLY when it is clearly and entirely unrelated to the learning goal (e.g., chatting about lunch, asking the AI to tell jokes, random conversation). For anything ambiguous — tangential questions, meta-questions about the subject, clarification requests, loosely related concepts — classify as on-topic.\n\nExamples with goal \"solve quadratic equations using the discriminant\":\n- **ON-TOPIC:** \"What's a discriminant?\" (directly relevant)\n- **ON-TOPIC:** \"Is this like the quadratic formula?\" (tangential but related)\n- **ON-TOPIC:** \"I don't understand the question\" (meta but goal-relevant)\n- **ON-TOPIC:** \"Can you explain what ax² means?\" (foundational concept)\n- **ON-TOPIC:** \"My teacher explained this differently, she said...\" (mixed, has relevant content)\n- **OFF-TOPIC:** \"What's for lunch?\" (clearly unrelated)\n- **OFF-TOPIC:** \"Tell me a joke\" (clearly unrelated)\n- **OFF-TOPIC:** \"What's the capital of France?\" (clearly unrelated)\n\n**Why no special fallback is needed:**\n\nThe warning/escalation system is tolerant of edge cases by design:\n- A single misclassification only produces a student-visible warning (no teacher involvement)\n- The student's next on-topic message resets the counter (REQ-AI-007)\n- Teacher alerts only fire on *consecutive* off-topic messages after a warning\n- False negatives (missing truly off-topic messages) are harmless — the inactivity system (REQ-AI-009) catches students who aren't making progress regardless\n\nIn short: bias toward on-topic, and let the multi-step escalation system absorb any remaining classification noise."
      },
      {
        "question": "Should students be able to see their completed sub-tasks for percentage goals (e.g., 'Equations solved: 1, 2' not just '66%'), or only the aggregate percentage?",
        "answer": "The PRD specifies percentage-only display for students. Implement just the progress bar with percentage — no sub-task breakdown on the student side.\n\n**What the PRD says:**\n\n- REQ-GOAL-001: Percentage goals are \"displayed as progress bar, e.g., 0%, 33%, 66%, 100%\"\n- REQ-GOAL-005: \"The student shall always see their current progress indicator on the chat screen\"\n- AC-GOAL-01: \"their progress shows 33%\" — the acceptance criterion validates against the percentage, not sub-task labels\n\nThe PRD consistently describes the student-facing progress as a percentage/progress bar, never as an itemized checklist.\n\n**Sub-task detail is explicitly a teacher feature:**\n\n- REQ-DASH-007: The teacher detail view shows \"key messages that contributed to goal progress\"\n- REQ-DASH-008: The teacher detail view presents \"task-solution pairs\" or \"aggregated message summaries\"\n\nThis granular breakdown is scoped to the teacher dashboard, not the student chat screen.\n\n**Why this is sufficient for students:**\n\n- Students already see which of their messages advanced progress (REQ-CHAT-007: green border highlighting). This implicitly tells them which tasks they've completed.\n- The AI tutor is guiding them through the goal (REQ-AI-002), so it can reference what's left (\"You've solved 2 equations so far, try a third one\") in its chat responses.\n- Keeping the student UI to just a progress bar keeps the chat screen simple and avoids duplicating information already conveyed through message highlighting and AI guidance.\n\n**Implementation: student sees a progress bar + percentage. That's it.** The AI's chat responses and green-bordered messages provide the sub-task context naturally."
      },
      {
        "question": "What is the expected behavior when a teacher confirms a goal interpretation but then discovers it was wrong during a live session? Can they create a new group with the same name, or is there a workaround?",
        "answer": "The PRD intentionally makes this a \"create a new group\" situation. No special recovery mechanism is needed — the workaround is already built into the design.\n\n**What the PRD explicitly rules out:**\n- Out of Scope #4: \"Group editing after confirmation — Goals are immutable once confirmed\"\n- Out of Scope #5: \"Group deletion/disbanding — Groups are permanent\"\n- REQ-GROUP-016: \"Groups are permanent once created. Teachers cannot disband or delete groups.\"\n\n**The built-in workaround — create a new group:**\n- Nothing in the PRD prevents duplicate group names. REQ-GROUP-001 only requires a name and goal description; REQ-GROUP-002 generates a *unique join code*, not a unique name. The teacher can create \"A2 - quadratic equations 1\" again with a corrected goal.\n- Students scan a new QR code to join the new group. Since each group has its own join code (REQ-GROUP-002), there's no conflict.\n- Device binding is per-group (REQ-GROUP-012: \"one device can join a *specific group* only once\"), so students can join the new group from the same device with no issues.\n\n**Implementation: do NOT enforce unique group names.** Group names are display labels, not identifiers. The unique join code is the real identifier.\n\n**The old group just sits there:**\n- It remains in the teacher's group list (REQ-GROUP-017) but with 0 active students once everyone moves to the new group.\n- This is acceptable for a prototype. The teacher's group list shows creation date (REQ-GROUP-003), so they can easily distinguish the old vs. new group.\n\n**Why this is fine for a prototype:**\n- The confirmation flow (REQ-GROUP-005 through REQ-GROUP-007) with unlimited rejection/resubmission already minimizes this scenario — the teacher reviews the interpretation *before* students can join.\n- The pre-confirmation safeguard means the \"wrong goal discovered mid-session\" case should be rare. When it does happen, creating a new group takes seconds and students just scan a new QR code.\n- Building group editing or archival features would be over-engineering for a prototype."
      },
      {
        "question": "What should happen if the LLM API becomes unavailable mid-session (after students have joined)? Should chat continue without AI guidance, or should the session pause?",
        "answer": "The student should stay in the chat but receive an error message for the failed AI response, with the ability to retry by sending another message. The session should NOT pause or lock.\n\n**Why not pause/lock the session:**\n- The PRD architecture is fundamentally \"student sends message → AI responds.\" There's no concept of a session-level state toggle. The student's chat and progress bar remain intact — only the AI response for that specific message fails.\n- REQ-RT-002 establishes the principle of graceful degradation for connection issues. The same principle applies here: degrade gracefully, don't hard-stop.\n- Pausing the session would require a new state machine and UI (not in the PRD) and would be over-engineering for a prototype.\n\n**Recommended implementation — simple per-message error handling:**\n\n1. Student sends a message → it appears in the chat (their message is persisted regardless).\n2. The backend calls the LLM API and it fails (timeout, 5xx, rate limit, etc.).\n3. Display an error message in the chat like: \"Sorry, I'm having trouble responding right now. Please try again in a moment.\"\n4. The student's message gets NO classification (no green border, no off-topic warning) — skip all AI-dependent processing for that message.\n5. Chat input remains enabled — the student can send another message, which triggers a new API call.\n6. No progress update occurs for the failed message (safe default — you can't award or penalize what you can't evaluate).\n\n**What about the inactivity timer?**\n- The student DID send a message, so the inactivity timer resets regardless of API failure (REQ-AI-009 is based on \"last message sent,\" not \"last successful AI response\").\n\n**What about repeated failures?**\n- Each failed message shows the same error response. No escalation or special handling needed for a prototype.\n- The teacher can see from the dashboard that a student's progress has stalled and can intervene in person — this is exactly the kind of scenario the physical teacher presence covers.\n\n**Precedent from the PRD:**\n- REQ-GROUP-009 handles API failure during goal creation with \"display a generic error message allowing the teacher to retry.\" The same pattern applies here: show an error, let the user retry. The only difference is the retry is implicit (send another message) rather than explicit (click a button)."
      },
      {
        "question": "For the 'activity resumption notification' (REQ-AI-012), what notification mechanism should be used - a toast, an alert in the same alert queue, or something else?",
        "answer": "Use a **transient toast notification** (auto-dismissing after 3-5 seconds) rather than adding to the alert queue.\n\n**Rationale:**\n1. **The resumption is purely informational** — Unlike inactivity/off-topic alerts (REQ-DASH-003, REQ-DASH-004) which require teacher action (\"mark as resolved\"), the resumption notification just informs the teacher \"problem resolved itself\" — no action needed.\n\n2. **Avoids alert clutter** — If added to the alert queue, teachers would need to manually dismiss these, creating unnecessary busywork. The student is now active, so there's nothing to resolve.\n\n3. **PRD language distinction** — REQ-AI-012 uses \"notify\" while REQ-DASH-003 uses \"alert indicator\". The PRD distinguishes between actionable alerts and informational notifications.\n\n**Implementation (simplest approach):**\n- Create a simple toast component (absolutely positioned, bottom-right or top-right)\n- Show for ~4 seconds with student nickname: \"Honza has resumed activity\"\n- Use theme colors (teal/tertiary for success/positive)\n- Push via SignalR along with other real-time updates\n- No persistence, no database storage for resumption notifications"
      },
      {
        "question": "What is the expected scale for concurrent groups? Should one teacher be able to run multiple groups simultaneously, and if so, should there be cross-group visibility on the dashboard?",
        "answer": "**Yes, a teacher can have multiple groups, but the dashboard is single-group-at-a-time. No cross-group aggregated view is needed.**\n\n**What the PRD establishes:**\n\n1. **Multiple groups per teacher** — REQ-GROUP-003 explicitly says: \"Teachers shall view a table listing all their groups with columns: name, creation date, active students count, overall progress.\" This is a group list/index page, not a single-group view. Groups are permanent (REQ-GROUP-016/017), so they accumulate over time.\n\n2. **Dashboard is scoped to one group** — REQ-DASH-001 says \"Teachers shall view all students **in a group**\" (singular). REQ-DASH-005 says \"a simple scrollable list of students, optimized for ~30 concurrent students\" — this is per-group, not across groups.\n\n3. **No cross-group aggregation specified** — Nothing in the PRD mentions cross-group alerts, combined student views, or multi-group dashboards.\n\n**Recommended implementation (two-level navigation):**\n\n- **Groups list page** (REQ-GROUP-003): Table showing all teacher's groups with name, creation date, active student count, and overall progress. This gives at-a-glance awareness of activity across groups.\n- **Group detail/dashboard page** (REQ-DASH-001–009): Single-group monitoring view with the student list, alerts, and expandable detail rows.\n\n**Scale assumptions for this prototype:**\n- No limit on groups per teacher (but realistically a handful of active ones)\n- ~30 concurrent students per group (PT-003)\n- The \"active students count\" column on the groups list page gives teachers enough cross-group visibility to know which group needs attention, without building a complex aggregated dashboard"
      },
      {
        "question": "How should the system handle browser tab/window close or device going to sleep? Should the inactivity timer only count when the session is actively connected, or also when disconnected?",
        "answer": "**The inactivity timer should run continuously based on last message timestamp, regardless of connection state.** A disconnected student is arguably *more* inactive than a connected-but-silent one.\n\n**Rationale from the PRD:**\n\nREQ-AI-009 is unambiguous: \"inactivity based on a fixed 10-minute timeout **from the last message sent**.\" The trigger is purely temporal — time since last message — not connection-dependent. The timer is server-side (checking a persisted timestamp), so it naturally keeps ticking whether the student is connected or not.\n\n**How it works in practice:**\n\n1. **Student sends a message** — server records `LastMessageAt = DateTime.UtcNow` in the database (REQ-GOAL-006 already requires immediate persistence).\n2. **Server-side timer/check** periodically evaluates: `DateTime.UtcNow - student.LastMessageAt > 10 minutes` → trigger inactivity alert.\n3. **Student closes laptop at minute 3** — SignalR disconnects. Timer keeps running. At minute 10, alert fires. This is correct behavior: the teacher needs to know this student hasn't done anything for 10 minutes, regardless of why.\n4. **Student reopens laptop at minute 15** — Blazor Server reconnects (REQ-RT-002), session restores (REQ-RT-003). When they send a message, REQ-AI-012 fires the resumption toast notification.\n\n**Why connection-aware timing would be wrong:**\n- It would add complexity for no pedagogical benefit — a teacher doesn't care *why* a student is inactive, just that they are\n- A student closing their laptop to avoid work is the exact scenario inactivity detection is meant to catch\n- Blazor Server's reconnection window is brief (~30 seconds default) — pausing the timer during disconnection could miss genuine inactivity\n- The PRD says \"from the last message sent\" — not \"from the last message sent while connected\"\n\n**No special handling needed for disconnect/sleep:**\n- Blazor Server already handles reconnection (REQ-RT-002) with a built-in overlay (AC-RT-01)\n- Chat history restores from the server on reconnection (REQ-RT-003)\n- Session restores via device-binding (REQ-GROUP-013)\n- The server-side inactivity check doesn't depend on SignalR connection state at all"
      },
      {
        "question": "For the QR code join URL, should it be a short URL or contain the full join path? What domain/base URL should be used for the QR code content?",
        "answer": "**Use a full application URL with a short random join code. The base URL should come from application configuration.**\n\n**URL structure:** `{BaseUrl}/join/{JoinCode}`\n\n- **Example:** `https://myapp.azurewebsites.net/join/A7k3Xm`\n- **JoinCode:** A short alphanumeric code (6-8 characters), generated server-side and unique per group. Short enough to keep the QR code simple/scannable, long enough to avoid collisions.\n\n**Why this approach:**\n\n1. **No URL shortener needed** — The URL is already short. A path like `/join/A7k3Xm` adds minimal length. QR codes handle URLs of this length easily at standard sizes. Adding a URL shortener would introduce an external dependency for no real benefit in a prototype.\n\n2. **Base URL from configuration** — Store it in `appsettings.json` (e.g., `Application:BaseUrl`), overridable via environment variables. This naturally supports:\n   - Local dev via Docker: `http://localhost:5000`\n   - Azure deployment: `https://{appname}.azurewebsites.net`\n   - Custom domain if ever added later\n\n3. **Join code, not group ID** — Use a random code rather than exposing database IDs in URLs. This is a minor security best practice and produces shorter, cleaner URLs than GUIDs.\n\n**QR code generation:**\n- Use QRCoder library (already listed in PRD assumptions)\n- Generate the QR code server-side as SVG or PNG\n- The QR code encodes the full URL: `{BaseUrl}/join/{JoinCode}`\n- Display it on the group detail page for the teacher to project/share\n\n**Route handling:**\n- `/join/{JoinCode}` — Blazor page that looks up the group by join code\n  - If group exists and is confirmed → show nickname entry (REQ-GROUP-011)\n  - If group exists but unconfirmed → show \"not yet available\" message (AC-GROUP-05)\n  - If device already joined this group (localStorage check) → restore session (REQ-GROUP-013)\n  - If code invalid → show error\n\n**No special short-URL service or redirect layer is needed for this prototype.**"
      },
      {
        "question": "What is the maximum expected length for goal descriptions that the AI must parse? Are there any constraints on goal complexity (e.g., max number of steps for percentage goals)?",
        "answer": "**No explicit constraints are specified in the PRD. Apply sensible prototype defaults: reasonable text length limit on the input field and a practical step cap for UI usability.**\n\n**What the PRD tells us:**\n\n- Goal descriptions are free-text English entered by teachers (REQ-GROUP-001)\n- The AI parses them into binary or percentage goals (REQ-GROUP-004, REQ-GOAL-001)\n- For percentage goals, the AI determines discrete steps (REQ-GOAL-003), e.g., \"solve 3 equations\" = 3 steps at 33.3% each\n- The teacher reviews and confirms the AI's interpretation (REQ-GROUP-005/006), acting as a human guardrail against bad parses\n- Teachers can reject and resubmit indefinitely (REQ-GROUP-007)\n\n**Recommended prototype constraints:**\n\n1. **Goal description length:** Cap the input at ~500 characters. This is plenty for educational goals (the PRD examples are ~100 characters) and keeps AI prompt tokens reasonable. Enforce via `maxlength` on the textarea — no need for server-side validation beyond basic sanity.\n\n2. **Step count for percentage goals:** No hard cap in the code, but the AI prompt should instruct a practical range (e.g., 2–10 steps). Rationale:\n   - Too few (1 step) = should be binary instead\n   - Too many (20+ steps) = progress bar becomes unreadable, progress percentages become meaninglessly granular\n   - The confirmation flow (REQ-GROUP-005/006) is the real safety net — if the AI returns 15 steps, the teacher can reject and simplify\n\n3. **No need to over-engineer validation** — The teacher confirmation loop (enter goal → AI interprets → teacher accepts/rejects → repeat) already handles edge cases:\n   - Overly complex goal? AI breaks it into too many steps → teacher rejects → rewrites simpler goal\n   - Ambiguous goal? AI misinterprets → teacher rejects → clarifies\n   - The PRD explicitly allows unlimited resubmissions (REQ-GROUP-007)\n\n**For the AI prompt design**, include guidance like:\n- \"Break the goal into 2–10 discrete steps. If the goal has more than 10 distinct sub-tasks, group related items into logical stages.\"\n- \"If the goal describes a single achievement without countable sub-tasks, classify as binary.\"\n\n**Summary:** Don't add hard validation beyond a reasonable input length. The teacher confirmation loop is the intended quality control mechanism. Keep the AI prompt tuned to produce sensible step counts (2–10 range) and trust the teacher to reject outliers."
      }
    ],
    "source": "agent"
  },
  {
    "round": 2,
    "questions": [
      {
        "question": "For speech-to-text dictation (REQ-CHAT-005), should this use browser-native Web Speech API, or an external speech-to-text service? If external, which service should be used and is there a budget/API key available?",
        "reason": "REQ-CHAT-005 requires voice dictation but doesn't specify the implementation approach. Browser-native API has varying browser support and quality. An external service requires additional API keys and cost considerations."
      },
      {
        "question": "For the expandable student detail view (REQ-DASH-007/008), should the teacher be able to see the FULL chat history, or only the key messages that contributed to progress? If only key messages, what about viewing context around those messages?",
        "reason": "The PRD says 'key messages that contributed to goal progress' but teachers might need to see surrounding context to understand the student's thought process. The scope of what's visible affects both privacy and debugging utility."
      },
      {
        "question": "Should there be any rate limiting on student messages to prevent spam or abuse of the LLM API (e.g., max messages per minute)?",
        "reason": "Students could potentially spam messages, incurring significant LLM API costs and potentially degrading experience. The PRD doesn't specify any message rate limits or abuse prevention mechanisms."
      },
      {
        "question": "For the inactivity timer (REQ-AI-009), when exactly does it start - when the student joins the group, or after they send their first message? Does viewing but not responding to the welcome message count as inactive?",
        "reason": "A student might join, see the welcome message, but never respond. The PRD says 'from the last message sent' but it's unclear what happens if no message has ever been sent. Does the join timestamp count as the starting point?"
      },
      {
        "question": "What LLM provider and model should be used (e.g., OpenAI GPT-4, GPT-3.5-turbo, Anthropic Claude, Azure OpenAI)? This affects API key setup, cost, and response quality.",
        "reason": "REQ-DEV-002 mentions 'LLM API key' but doesn't specify which provider. LLMTornado supports multiple providers. The choice affects token limits for goal interpretation, chat context window, and operational costs."
      },
      {
        "question": "For the TherapistTemplate visual theme reference, is there access to that codebase to extract exact styles, or should the implementation approximate based on the color palette and description provided in the PRD?",
        "reason": "REQ-THEME-001-005 reference TherapistTemplate but the project directory shows this is a greenfield project. Without access to TherapistTemplate's actual CSS/styles, the implementer would need to recreate the visual style from description alone, which may not match expectations."
      },
      {
        "question": "Should the AI tutor maintain conversation memory/context across the entire session, or use a sliding window of recent messages? What's the maximum context that should be sent to the LLM per request?",
        "reason": "Long tutoring sessions could accumulate significant chat history. The PRD doesn't specify how much conversation context the AI should consider when responding, which affects both response quality and LLM token costs."
      },
      {
        "question": "For device binding via localStorage (REQ-GROUP-013), what should happen if a student clears their browser data or uses a different browser on the same device? Can they rejoin with the same nickname, or is that session permanently lost?",
        "reason": "localStorage persistence is browser-specific and can be cleared by users. The PRD doesn't address session recovery if localStorage is lost, potentially leaving students unable to continue their progress."
      }
    ],
    "answers": [
      {
        "question": "For speech-to-text dictation (REQ-CHAT-005), should this use browser-native Web Speech API, or an external speech-to-text service? If external, which service should be used and is there a budget/API key available?",
        "answer": "Use browser-native Web Speech API. The PRD explicitly mandates this in REQ-CHAT-012: \"Students shall have the option to dictate messages via voice using the Web Speech API.\" Additionally, TC-004 explicitly acknowledges the browser support constraint: \"Voice dictation (Web Speech API) requires browser support (Chrome, Edge).\" No external speech service is required or budgeted for."
      },
      {
        "question": "For the expandable student detail view (REQ-DASH-007/008), should the teacher be able to see the FULL chat history, or only the key messages that contributed to progress? If only key messages, what about viewing context around those messages?",
        "answer": "[Agent uncertain] The PRD specifies a two-tier approach that depends on student state:\n\n**Default detail view (REQ-DASH-007/008):** Shows only key messages that contributed to goal progress, presented as task-solution pairs or aggregated message summaries. This is the primary view for both in-progress and completed students.\n\n**Full chat history access (REQ-GOAL-012, AC-GOAL-08):** Teachers explicitly \"retain full access to view chat history and progress details for completed students.\" AC-GOAL-08 acceptance criterion states: \"Given a student has completed their goal, when the teacher views their detail, then full chat history is visible.\"\n\n**For in-progress students**, the PRD only specifies key messages / task-solution pairs / aggregated summaries — not full chat access. No mention is made of surrounding context around key messages.\n\n**Recommendation for implementation:** The detail view should default to the curated \"key messages\" view (task-solution pairs / summaries) for all students, and additionally provide a \"View full chat\" option. For completed students this full view is explicitly required (REQ-GOAL-012). For in-progress students, it would be a reasonable addition but is not strictly required by the PRD — this is a product decision that should be clarified with the stakeholder."
      },
      {
        "question": "Should there be any rate limiting on student messages to prevent spam or abuse of the LLM API (e.g., max messages per minute)?",
        "answer": "No. The PRD explicitly lists \"Rate limiting / abuse prevention\" as out of scope for this prototype. No rate limiting implementation is needed."
      },
      {
        "question": "For the inactivity timer (REQ-AI-009), when exactly does it start - when the student joins the group, or after they send their first message? Does viewing but not responding to the welcome message count as inactive?",
        "answer": "[Agent uncertain] The PRD is ambiguous on this point and needs a product decision.\n\nREQ-AI-009 states the timeout is \"from the last message sent.\" Strictly interpreted, if a student has never sent a message, there is no \"last message sent\" to measure from — meaning the inactivity timer would never start, and a student who joins and never types would never trigger an alert. This is almost certainly not the intended behavior, since a student who joins and does nothing is the most obvious case of inactivity.\n\n**Recommended implementation (pending stakeholder confirmation):** Treat the student's join timestamp (when they enter their nickname and see the welcome message) as the starting point for the first inactivity window. After the student sends their first message, subsequent inactivity timers measure from the last message sent, as the PRD specifies. This way:\n\n- Student joins → 10 min with no messages → inactivity alert fires\n- Student sends a message → 10 min with no further messages → inactivity alert fires\n\nThis aligns with the spirit of the assignment.md (\"If the student is not working... alerts the teacher\") and is the most useful behavior for teachers."
      },
      {
        "question": "What LLM provider and model should be used (e.g., OpenAI GPT-4, GPT-3.5-turbo, Anthropic Claude, Azure OpenAI)? This affects API key setup, cost, and response quality.",
        "answer": "The PRD intentionally does NOT specify a particular LLM provider or model. Instead, it requires provider/model configurability:\n\n- **TC-006** (line 314): \"LLM provider and model shall be configurable via application settings.\"\n- **Assumptions** (line 332): \"Single LLM provider with configurable API key\"\n- **REQ-DEPLOY-020** (line 290): `.env.example` documents a generic \"LLM API key\" — not provider-specific.\n\nThe implementation should use **LLMTornado** (per assignment.md and REQ line 267), which is a multi-provider abstraction library. The architecture should allow the provider and model to be set via configuration (e.g., environment variables or appsettings.json), so any LLMTornado-supported provider can be swapped in.\n\n**This is a product/operational decision, not a technical one.** The stakeholder needs to decide which provider to use based on their available API keys and budget. The code should be provider-agnostic via LLMTornado, with the specific provider/model specified in configuration. The `.env.example` should document which environment variables to set (e.g., `LLM_PROVIDER`, `LLM_MODEL`, `LLM_API_KEY`)."
      },
      {
        "question": "For the TherapistTemplate visual theme reference, is there access to that codebase to extract exact styles, or should the implementation approximate based on the color palette and description provided in the PRD?",
        "answer": "**Yes, the TherapistTemplate codebase is fully accessible** at `/Users/mu/Coding/TherapistTemplate` and exact styles can be extracted.\n\nThe main CSS file is at `/Users/mu/Coding/TherapistTemplate/frontend/src/index.css` (499 lines) and contains complete, production-ready styles including:\n\n**Exact values extractable:**\n- Full color palette with shades (primary-50 through primary-900 for each color)\n- CSS custom properties for all theme values\n- Glow shadow definitions (e.g., `--shadow-glow-pink: 0 0 20px rgba(255, 107, 157, 0.4)`)\n- Animation keyframes (`glow-pulse`, `float`)\n- Component classes: `.glow-container`, `.inner-card`, `.glow-button`, `.gradient-text`, `.nav-link`\n- Typography setup for Outfit/Inter fonts\n- Light mode overrides (though PRD specifies dark-only)\n\n**Recommendation:** Copy the relevant CSS variables and component styles directly from TherapistTemplate rather than approximating. This ensures pixel-perfect matching. The PRD's appendix values align exactly with the source CSS."
      },
      {
        "question": "Should the AI tutor maintain conversation memory/context across the entire session, or use a sliding window of recent messages? What's the maximum context that should be sent to the LLM per request?",
        "answer": "**The PRD does not specify a context strategy or token limit.** This is a technical implementation decision left to the implementer. Here is the recommended approach based on the PRD's functional requirements and constraints:\n\n**Use full conversation history, with a pragmatic upper bound.**\n\nThe PRD requirements that demand full context awareness:\n1. **Progress tracking is cumulative and order-agnostic** (REQ-GOAL-007) — the AI must know what steps have already been completed across the entire session to avoid re-crediting work or missing progress.\n2. **Off-topic warning resets** (REQ-AI-007) — the AI needs to understand the flow of on-topic vs off-topic messages to correctly manage the warning cycle.\n3. **\"Guide without giving direct answers\"** (REQ-AI-002/004) — the AI must remember what hints it already gave to avoid repeating itself or accidentally revealing answers it previously withheld.\n4. **Teacher detail view shows key progress messages** (REQ-DASH-007/008) — the AI generates summaries, which benefits from full conversation awareness.\n\n**Why a sliding window alone is risky:** If the window drops early messages where a student solved step 1, the AI might re-award that step or fail to recognize the student already demonstrated understanding.\n\n**Recommended implementation approach:**\n- **Send full conversation history** to the LLM by default.\n- **Set a practical ceiling of ~50 messages (or ~8,000 tokens of conversation history)** as a soft limit. For a tutoring session targeting ~30 students with an educational exercise, conversations are unlikely to exceed this in practice (PT-002 requires <5s response time, and PT-003 targets ~30 concurrent students).\n- **If a session exceeds the limit:** summarize older messages into a compact context block (completed steps, key decisions, current progress state) and prepend it to the recent message window. This preserves critical state without sending the full raw history.\n- **Always include in every request regardless of windowing:** the system prompt with goal definition, current progress state (steps completed), and the off-topic warning counter. This structured metadata acts as a \"memory anchor\" so the AI never loses track of progress even if raw messages are truncated.\n\nThis is a prototype (no automated testing required, ~30 students per group), so optimizing for simplicity: start with full history, and only implement summarization if response times degrade in practice."
      },
      {
        "question": "For device binding via localStorage (REQ-GROUP-013), what should happen if a student clears their browser data or uses a different browser on the same device? Can they rejoin with the same nickname, or is that session permanently lost?",
        "answer": "**The PRD explicitly addresses this and considers it acceptable for a prototype.**\n\n**TC-005** states: *\"LocalStorage device-binding can be bypassed by clearing browser data (acceptable for prototype).\"*\n\nCombined with **REQ-GROUP-015** (*\"If a student accesses the group from a different device, they are treated as a new student and must enter a new nickname\"*), the behavior is clear:\n\n**If localStorage is lost (cleared data, different browser, incognito), the student is treated as a brand new student.** They must:\n1. Enter a new nickname — they **cannot** reuse their old nickname because **REQ-GROUP-011a** enforces uniqueness within a group. Their old nickname is still \"taken\" by the orphaned server-side session.\n2. Start with fresh progress (0%).\n3. Their old session's chat history and progress remain visible to the teacher on the dashboard (as an apparently inactive student).\n\n**The old session is effectively orphaned, not recoverable.** There is no session-recovery mechanism in the PRD, and none should be built — the PRD explicitly accepts this limitation.\n\n**Implementation implication:** No special handling is needed. The implementer should simply treat a missing localStorage token the same as a first-time visitor. The orphaned session will eventually trigger an inactivity alert (REQ-AI-009) after 10 minutes, which the teacher can resolve manually."
      }
    ],
    "source": "agent"
  },
  {
    "round": 3,
    "questions": [
      {
        "question": "For the Web Speech API dictation (REQ-CHAT-005), should there be a visual indicator (microphone icon, recording state) and what happens if the browser doesn't support Web Speech API - should a fallback message be shown or should the dictation option be hidden entirely?",
        "reason": "TC-004 acknowledges browser support is required but doesn't specify the UX for unsupported browsers. This affects implementation of the dictation feature."
      },
      {
        "question": "For the inactivity timer start condition: should the timer begin at student join time (when they enter nickname) or only after they send their first message? A student who joins and never types would never trigger an inactivity alert under a strict reading of REQ-AI-009.",
        "reason": "Clarification Round 2 flagged this as [Agent uncertain] and recommended starting at join time, but this needs stakeholder confirmation since it's a behavior that affects teacher alerting."
      },
      {
        "question": "Should in-progress students' detail views include a 'View full chat' option (in addition to the key messages view), or is full chat history access only required for completed students (REQ-GOAL-012)?",
        "reason": "Clarification Round 2 flagged this as [Agent uncertain]. The PRD is explicit about full chat for completed students but ambiguous for in-progress students. Teachers may need full context to help struggling students."
      },
      {
        "question": "For particles background (REQ-THEME-002), which specific Blazor-compatible particles library should be used? Options include tsparticles (via JS interop), a Blazor-native port, or a custom implementation.",
        "reason": "The PRD says 'using a particles library compatible with Blazor' but doesn't specify which one. This affects bundle size, performance, and implementation complexity."
      },
      {
        "question": "What should happen to an orphaned student session (localStorage cleared, student rejoins with new nickname) on the teacher dashboard - should it remain visible indefinitely, or is there an implicit cleanup after some inactivity period?",
        "reason": "The clarifications confirm orphaned sessions trigger inactivity alerts, but don't specify whether teachers need to manually resolve all orphaned sessions or if they get cleaned up. This could clutter the dashboard over time."
      },
      {
        "question": "For the AI tutor's conversation context strategy, should the implementation use full conversation history by default with a ~50 message / ~8000 token soft cap, implementing summarization only if response times degrade?",
        "reason": "Clarification Round 2 recommended this approach but framed it as a recommendation. Confirming this strategy is acceptable avoids over-engineering or under-engineering the context management."
      }
    ],
    "answers": [
      {
        "question": "For the Web Speech API dictation (REQ-CHAT-005), should there be a visual indicator (microphone icon, recording state) and what happens if the browser doesn't support Web Speech API - should a fallback message be shown or should the dictation option be hidden entirely?",
        "answer": "The PRD explicitly addresses both questions:\n\n1. **Visual indicator**: Yes. REQ-CHAT-013 states \"A microphone button shall toggle voice input mode\" - so a microphone button/icon is required. The recording state should be visually indicated when the button is toggled.\n\n2. **Unsupported browsers**: Hide the dictation button entirely. REQ-CHAT-015 explicitly states: \"Voice dictation requires browser support (Chrome, Edge recommended). Unsupported browsers shall hide the dictation button.\" TC-004 reaffirms this: \"Unsupported browsers hide the dictation button.\"\n\n**Implementation approach (simple):**\n- Check `'webkitSpeechRecognition' in window || 'SpeechRecognition' in window` on page load\n- If unsupported: don't render the microphone button at all\n- If supported: show microphone button that toggles between idle/recording states (simple color change or animation is sufficient)\n- No fallback message needed - just hide the feature silently"
      },
      {
        "question": "For the inactivity timer start condition: should the timer begin at student join time (when they enter nickname) or only after they send their first message? A student who joins and never types would never trigger an inactivity alert under a strict reading of REQ-AI-009.",
        "answer": "The PRD already explicitly answers this. The timer starts at join time, not after the first message.\n\n**REQ-AI-013** states: \"The inactivity timer starts when the student joins the group (enters nickname and sees welcome message). After the first message, subsequent timers measure from the last message sent.\"\n\nThis is a two-phase rule:\n1. **Before any messages**: Timer starts at join time (nickname entered, welcome message displayed)\n2. **After first message**: Timer resets to measure from last message sent\n\nSo a student who joins and never types **will** trigger an inactivity alert after 10 minutes. This is the intended behavior — teachers need to know if a student joined but isn't engaging.\n\nNo stakeholder confirmation needed; this is already specified."
      },
      {
        "question": "Should in-progress students' detail views include a 'View full chat' option (in addition to the key messages view), or is full chat history access only required for completed students (REQ-GOAL-012)?",
        "answer": "[Agent uncertain] The PRD contains a contradiction that genuinely needs stakeholder clarification.\n\n**The conflict:**\n- **REQ-DASH-015** states: \"The detail view shall work identically for in-progress and completed students.\"\n- **REQ-DASH-014** states: \"For students who have completed their goal, the teacher shall have access to view full chat history in the detail view.\"\n\nIf the detail view works *identically* for both (REQ-DASH-015), and completed students get full chat history (REQ-DASH-014), then logically in-progress students should also get full chat history. But REQ-DASH-014 specifically scopes full chat to completed students, which implies a distinction.\n\n**Recommended interpretation:** Full chat history should be available for **both** in-progress and completed students. Rationale:\n1. REQ-DASH-015 explicitly says \"identically\" — the stronger, more general requirement\n2. REQ-DASH-014 likely calls out completed students to ensure chat remains viewable *after* the student's input is disabled, not to restrict it from in-progress students\n3. Teachers need full context to help struggling students (the core use case for monitoring)\n4. There's no technical or UX reason to withhold chat history from an in-progress student's detail view\n\nHowever, this is a genuine ambiguity in the PRD and should be confirmed with the stakeholder. The implementation cost difference is negligible either way."
      },
      {
        "question": "For particles background (REQ-THEME-002), which specific Blazor-compatible particles library should be used? Options include tsparticles (via JS interop), a Blazor-native port, or a custom implementation.",
        "answer": "The PRD explicitly specifies **tsparticles** (via JS interop).\n\n**REQ-THEME-004** states: \"The application shall feature an animated particle background using tsparticles or equivalent library.\"\n\nThe Assumptions section also lists: \"Particle animation via tsparticles.\"\n\n**Recommended: tsparticles via JS interop.** Rationale:\n1. The PRD names it directly as the primary choice\n2. tsparticles is the mature, widely-used library with built-in support for all the specified behaviors (mouse grab, click push, link lines with triangles) — see REQ-THEME-005 and REQ-THEME-007\n3. JS interop from Blazor Server is straightforward — load the tsparticles script, call `tsParticles.load()` from a small JS interop wrapper\n4. A Blazor-native port would have fewer features and less community support\n5. A custom implementation would be unnecessary effort for a prototype\n\n**Implementation approach:**\n- Include tsparticles via CDN or npm bundle\n- Create a small `particles.js` interop file that calls `tsParticles.load()` with the theme config (pink/purple/teal colors, grab/push interactivity, triangle links)\n- Invoke it from Blazor via `IJSRuntime.InvokeVoidAsync` on page load"
      },
      {
        "question": "What should happen to an orphaned student session (localStorage cleared, student rejoins with new nickname) on the teacher dashboard - should it remain visible indefinitely, or is there an implicit cleanup after some inactivity period?",
        "answer": "Orphaned sessions remain visible indefinitely. There is no cleanup mechanism, and the PRD does not call for one.\n\n**REQ-GROUP-022** explicitly states: \"The orphaned session remains visible to the teacher on the dashboard.\"\n\n**REQ-GROUP-024/025** reinforce that groups (and by extension their student data) are permanent and visible indefinitely.\n\n**For the prototype, this is fine.** The PRD explicitly accepts this as a known limitation:\n- REQ-GROUP-023 acknowledges the localStorage bypass issue and calls it \"acceptable for the prototype\"\n- The dashboard targets ~30 concurrent students (PT-004), so even with a few orphaned sessions, clutter is manageable\n- No deletion/cleanup features are in scope (groups can't be deleted, teacher has no manual override tools)\n\n**What happens in practice with orphaned sessions:**\n1. The orphaned session sits on the dashboard with its last-known progress state\n2. It triggers an inactivity alert after 10 minutes (per REQ-AI-015)\n3. The teacher resolves the alert — that's it\n4. No further inactivity alerts fire unless the teacher resolves (REQ-AI-016)\n5. The session remains as a stale entry on the student list\n\nImplementing session cleanup or archiving would be a post-prototype enhancement and is not required for this build."
      },
      {
        "question": "For the AI tutor's conversation context strategy, should the implementation use full conversation history by default with a ~50 message / ~8000 token soft cap, implementing summarization only if response times degrade?",
        "answer": "Yes, this strategy is explicitly specified in the PRD. REQ-AI-024 requires \"full conversation context\" and REQ-AI-026 defines the soft cap as \"~50 messages or ~8000 tokens of history\" with summarization only when exceeded. Implementation should: (1) pass full message history by default, (2) include goal definition, progress state, and off-topic counter per REQ-AI-025, (3) implement summarization later only if the soft cap is routinely exceeded and performance degrades. For the prototype, start with full history only — defer summarization until needed."
      }
    ],
    "source": "agent"
  },
  {
    "round": 4,
    "questions": [
      {
        "question": "For completed students (REQ-GOAL-007), should the chat input be disabled or should students be allowed to continue chatting with the AI tutor even after reaching 100% / binary complete? The PRD mentions 'input is disabled' for completed students but doesn't specify if this applies immediately upon goal completion.",
        "reason": "REQ-GOAL-007 says 'the student's input is disabled' for completed students, but it's unclear whether this is immediate (blocking further interaction) or if there's a transition period. This affects both UX (can students ask follow-up questions?) and implementation (chat input component state management)."
      },
      {
        "question": "Should the teacher dashboard auto-scroll or use sticky positioning to keep alert indicators visible when scrolling through a list of ~30 students? The PRD specifies a 'simple scrollable list' but doesn't address how alerts should remain visible.",
        "reason": "With ~30 students (PT-003), the list may require scrolling. If a student with an alert is off-screen, the teacher might miss it. The PRD specifies alert indicators (REQ-DASH-003, REQ-DASH-004) but not their visibility behavior during scrolling."
      },
      {
        "question": "For the expanded student detail view (REQ-DASH-007/008), should the 'key messages that contributed to progress' be determined by the AI during message processing, or computed on-demand when the teacher expands the view?",
        "reason": "This affects database schema design and performance. If computed on-demand, there's latency when expanding. If pre-computed, the AI response must include metadata about which messages are 'key'. The PRD doesn't specify the mechanism for identifying/storing key messages."
      },
      {
        "question": "How should the progress bar handle non-integer percentages for goals with step counts that don't divide evenly into 100? For example, a 7-step goal would have steps at ~14.3% increments. Should values be rounded, or should exact fractions be shown?",
        "reason": "REQ-GOAL-002 shows examples like '33%' for 3-step goals, but many step counts (4, 6, 7, etc.) produce non-round percentages. Displaying '14.285714%' would be awkward. The PRD doesn't specify rounding behavior or display precision."
      },
      {
        "question": "For the AI's 2-10 step recommendation range for percentage goals (REQ-GROUP-005), should the implementation enforce this as a hard limit, or just guidance in the AI prompt? What happens if the AI returns 1 step (should be binary) or 15 steps?",
        "reason": "The clarifications mention '2-10 steps recommended' as AI prompt guidance, but don't clarify whether the backend should validate/reject interpretations outside this range, or if the teacher confirmation step is the only safeguard."
      },
      {
        "question": "Should there be any visual distinction between a student who completed their goal (100%/checkmark) versus a student who is still in progress? E.g., a 'completed' badge or different row styling on the dashboard?",
        "reason": "The PRD specifies progress display (REQ-DASH-002) but doesn't explicitly describe differentiated styling for completed students. This affects dashboard scannability for teachers trying to identify who still needs attention."
      }
    ],
    "answers": [
      {
        "question": "For completed students (REQ-GOAL-007), should the chat input be disabled or should students be allowed to continue chatting with the AI tutor even after reaching 100% / binary complete? The PRD mentions 'input is disabled' for completed students but doesn't specify if this applies immediately upon goal completion.",
        "answer": "The chat input should be disabled IMMEDIATELY upon goal completion. The PRD is explicit and consistent on this:\n\n1. **REQ-GOAL-011**: \"Upon goal completion, the chat input shall be disabled—the student cannot send further messages.\"\n\n2. **AC-GOAL-05/06**: Both acceptance criteria state that when progress reaches 100% or binary complete, \"they see a completion message and chat input is disabled\" — these are simultaneous outcomes.\n\n3. **Out of Scope #19**: Explicitly lists \"Continued chat after goal completion — Chat disabled upon completion\" — confirming no follow-up questions are allowed.\n\n4. **Business Rules Summary**: \"Goal completion behavior | Chat disabled\"\n\n**Implementation approach**: When the AI response indicates 100% progress (or binary complete), the same response that updates progress should also set a completion flag. The chat input component should check this flag and disable the input field. Display a completion message simultaneously. No transition period, no grace period for follow-up questions."
      },
      {
        "question": "Should the teacher dashboard auto-scroll or use sticky positioning to keep alert indicators visible when scrolling through a list of ~30 students? The PRD specifies a 'simple scrollable list' but doesn't address how alerts should remain visible.",
        "answer": "The PRD does NOT specify any special scroll behavior for alerts. The answer from the PRD is: use a simple scrollable list with no sticky positioning or auto-scrolling for alerts. Here's why this is sufficient and what the PRD does specify instead:\n\n1. **REQ-DASH-008** explicitly says \"a simple scrollable list of students, optimized for ~30 concurrent students.\" The word \"simple\" is intentional — no complex scroll behavior is required.\n\n2. **Alerts have a separate visibility mechanism**: REQ-DASH-011 specifies toast notifications that auto-dismiss after 4 seconds for activity resumption. These toasts are overlay-based and inherently visible regardless of scroll position. New inactivity/off-topic alerts (REQ-DASH-009, REQ-DASH-010) would similarly benefit from a toast announcement when they first appear.\n\n3. **Sort order as the practical solution**: The most natural implementation for a \"simple scrollable list\" is to sort students with active alerts to the top of the list. This keeps alerted students visible without any sticky/auto-scroll complexity. The PRD doesn't prescribe sort order, so sorting by alert-status-first, then by name or join time, is a reasonable default.\n\n**Recommended implementation (stays within PRD scope)**:\n- Simple scrollable list, no sticky headers or auto-scroll\n- Sort students with unresolved alerts to the top of the list\n- Use toast notifications (already required by REQ-DASH-011) to draw attention when new alerts appear\n- The teacher can then scroll to the relevant student or find them near the top\n\nThis keeps the implementation simple per the PRD's intent while ensuring alerts are practically visible."
      },
      {
        "question": "For the expanded student detail view (REQ-DASH-007/008), should the 'key messages that contributed to progress' be determined by the AI during message processing, or computed on-demand when the teacher expands the view?",
        "answer": "The key messages should be determined by the AI DURING message processing (pre-computed), not on-demand. The PRD already requires this data to be produced at message time — the detail view simply displays it.\n\n**Why pre-computed is the clear answer:**\n\n1. **REQ-CHAT-006 already requires per-message classification**: \"The AI shall classify each student message for progress contribution and on-topic/off-topic status.\" This means every message is already being tagged with whether it contributes to progress as part of the normal chat flow.\n\n2. **REQ-CHAT-007 requires immediate highlighting**: \"Messages that contribute to goal progress shall be visually highlighted (e.g., green border).\" The student sees the green border right away — so the \"contributes to progress\" flag must already exist on the message at processing time.\n\n3. **REQ-DASH-013 reuses this data**: The \"key messages that contributed to goal progress\" in the detail view are simply the messages already flagged as progress-contributing by the classification in REQ-CHAT-006. No separate computation is needed.\n\n4. **Task-solution pairs / summaries (Appendix point 6)**: \"Generate Summaries: Create task-solution pairs for teacher review.\" This is the only part that could arguably be on-demand, but it's more efficient and consistent to generate these summaries as part of the AI response when progress is actually awarded — the AI has full context at that moment.\n\n**Implementation approach:**\n- The AI response for each student message already includes: progress update, on-topic/off-topic classification, and whether the message contributed to progress (REQ-CHAT-006)\n- Store a `ContributesToProgress` boolean flag (or equivalent) on each message record\n- Optionally store a brief `ProgressSummary` text (the task-solution pair description) when progress is awarded\n- The detail view queries messages WHERE `ContributesToProgress = true`, which is a simple indexed database query — no AI call needed\n- This is fast (no latency on expand), consistent (same data the student saw), and doesn't consume additional LLM API calls"
      },
      {
        "question": "How should the progress bar handle non-integer percentages for goals with step counts that don't divide evenly into 100? For example, a 7-step goal would have steps at ~14.3% increments. Should values be rounded, or should exact fractions be shown?",
        "answer": "Round to the nearest whole integer for display. The PRD already demonstrates this convention and the underlying model is step-based, not percentage-based.\n\n**Evidence from the PRD:**\n\n1. **REQ-GOAL-001** shows the example: \"0%, 33%, 66%, 100%\" for a 3-step goal. Note that 1/3 is 33.33...% but the PRD writes \"33%\" — it's already using rounded integers in its own examples.\n\n2. **REQ-GOAL-003**: \"e.g., 'solve 3 equations' = 3 steps, each worth ~33%\". The tilde (~) explicitly signals approximation, confirming the PRD treats percentages as rounded display values.\n\n3. **REQ-GOAL-005**: \"Students see a progress bar with percentage.\" A progress bar is inherently visual/approximate — sub-percentage precision adds no value.\n\n4. **The real data model is steps, not percentages.** REQ-GOAL-007 says \"progress shall be tracked by total steps completed.\" The percentage is a derived display value: `Math.Round((completedSteps / totalSteps) * 100)`.\n\n**Implementation:**\n- Store: `CompletedSteps` (int) and `TotalSteps` (int) on the student progress record\n- Display: `Math.Round((double)completedSteps / totalSteps * 100)` → show as integer with \"%\" suffix\n- Progress bar width: use the exact fraction for smooth visual rendering (`width: calc(completedSteps / totalSteps * 100%)`)\n- Special case: always show 0% for no progress and 100% for all steps complete (avoid rounding artifacts like 99% or 1%)\n\nFor a 7-step goal: 0%, 14%, 29%, 43%, 57%, 71%, 86%, 100%."
      },
      {
        "question": "For the AI's 2-10 step recommendation range for percentage goals (REQ-GROUP-005), should the implementation enforce this as a hard limit, or just guidance in the AI prompt? What happens if the AI returns 1 step (should be binary) or 15 steps?",
        "answer": "Treat 2-10 as AI prompt guidance, NOT a hard backend validation limit. The teacher confirmation step (REQ-GROUP-007) is the primary safeguard. Here's the reasoning:\n\n**The PRD frames this as a recommendation, not a constraint:**\n\n1. **REQ-GOAL-003**: \"Target 2-10 steps; if more sub-tasks exist, group them into logical stages.\" The word \"target\" and the explicit \"if more sub-tasks exist\" clause show this is guidance for the AI's judgment, not a rigid rule. The PRD anticipates the AI might need to deviate.\n\n2. **REQ-GROUP-007/008**: The teacher MUST explicitly confirm the AI's interpretation before the group becomes joinable. This is the designed validation mechanism — a human reviews the step count and can reject it.\n\n3. **REQ-GROUP-009**: \"If the teacher rejects the interpretation, they shall be able to re-specify or edit the goal description and resubmit.\" This is the intended correction path for bad interpretations.\n\n**Implementation approach:**\n\n- **AI prompt**: Instruct the AI to target 2-10 steps, and to convert 1-step percentage goals to binary type. This handles the common case.\n- **Soft validation with auto-correction for 1 step**: If the AI returns a percentage goal with 1 step, automatically convert it to a binary goal before showing the teacher. A 1-step percentage goal is semantically identical to binary — there's no reason to let this through.\n- **No hard upper limit**: If the AI returns 12 or 15 steps, show it to the teacher as-is. The teacher can accept it if it makes sense for their goal, or reject and rephrase. The system works fine with any step count — the progress math is `completedSteps / totalSteps` regardless.\n- **Hard minimum of 2 for percentage goals**: A percentage goal with 0 steps is a division-by-zero bug. Enforce `totalSteps >= 2` for percentage type at the backend level — but this is a data integrity guard, not a business rule.\n\n**Summary**: Prompt guidance (2-10) + auto-convert 1-step to binary + hard minimum of 2 for percentage type (bug prevention) + teacher confirmation as the real safeguard. No hard upper cap."
      },
      {
        "question": "Should there be any visual distinction between a student who completed their goal (100%/checkmark) versus a student who is still in progress? E.g., a 'completed' badge or different row styling on the dashboard?",
        "answer": "No additional visual distinction beyond the existing progress indicators is required. The PRD specifies that completed students show 100% (filled progress bar) or a checkmark (REQ-GOAL-001, REQ-DASH-004), and that the \"completion state shall be visible\" (REQ-GOAL-012) — but this visibility is already achieved through those progress indicators themselves. No separate badge, row styling, or other differentiation is mentioned in the requirements.\n\nFor the SIMPLEST implementation: treat 100%/checkmark as sufficient visual distinction. The progress bar at 100% or a checkmark already clearly communicates completion. Adding extra badges or row styling is not in scope per current requirements — handle with basic existing UI elements only."
      }
    ],
    "source": "agent"
  },
  {
    "round": 5,
    "questions": [
      {
        "question": "For the inactivity timer, should the 10-minute timeout be configurable by the teacher per group, or is it a fixed system-wide value?",
        "reason": "The PRD specifies a fixed 10 minutes (REQ-AI-009), but educational goals vary widely in complexity. A complex math problem might require more think-time, while a quick vocabulary exercise might need a shorter timeout. The current implementation gives teachers no flexibility."
      },
      {
        "question": "When a teacher expands a student's detail view showing 'key messages that contributed to progress', what should be the format? Should it show (a) just the student messages that earned progress, (b) student message + AI response pairs, or (c) student message + a brief AI-generated summary of what was understood?",
        "reason": "REQ-DASH-007/008 mention 'key messages' and 'task-solution pairs' but don't specify the exact format. The clarification mentions pre-computing a 'ContributesToProgress' flag but doesn't clarify whether the AI response context is included in the detail view."
      },
      {
        "question": "Should the green border highlighting for progress-contributing messages (REQ-CHAT-007) be applied immediately when the AI response is received, or should there be any delay/animation to draw student attention to the achievement?",
        "reason": "This is a UX detail that affects the feeling of accomplishment. An abrupt border vs. a brief glow animation could significantly impact student motivation. The PRD specifies 'green border' but not the transition behavior."
      },
      {
        "question": "For the goal interpretation confirmation flow (REQ-GROUP-005), should the teacher see a preview of how progress increments will work (e.g., 'Step 1 complete = 33%, Step 2 = 66%, Step 3 = 100%'), or just the step count/names?",
        "reason": "This determines whether teachers can fully understand the progress granularity before confirming. The PRD says to show 'discrete steps identified' but doesn't specify the level of detail in the preview."
      },
      {
        "question": "What should happen if a student's device runs out of battery or loses connection during a session, reconnects hours later (after school ends), and sends a message? Should their session still be fully active, or should there be any session expiration?",
        "reason": "REQ-GROUP-016 says groups are permanent and the clarifications say orphaned sessions persist, but there's no mention of session TTL. A student could theoretically rejoin and chat hours or days later. Is this intended behavior?"
      },
      {
        "question": "For the off-topic warning indicator on student messages (REQ-CHAT-008), should it show any text explaining why the message was flagged, or just a visual indicator (icon/color)?",
        "reason": "The PRD says 'subtle warning indicator' but doesn't specify whether students should understand WHY their message was flagged. This affects whether students can learn from the warning or might be confused."
      }
    ],
    "answers": [
      {
        "question": "For the inactivity timer, should the 10-minute timeout be configurable by the teacher per group, or is it a fixed system-wide value?",
        "answer": "**Fixed system-wide value of 10 minutes. Not configurable per group or by teachers.**\n\nThe PRD is explicit on this point in multiple places:\n- **REQ-AI-012**: \"The system shall detect student inactivity based on a **fixed** 10-minute timeout.\"\n- **FC-001** (Fixed Configuration Values): \"Inactivity timeout: 10 minutes (**not configurable**).\"\n- **Out of Scope #10**: \"Configurable inactivity timeout — Fixed at 10 minutes.\"\n\nThis is a deliberate prototype simplification. While the reasoning in the question is valid (complex goals may need more think-time), per-group configurability is explicitly excluded from scope. Implement as a hardcoded 10-minute constant."
      },
      {
        "question": "When a teacher expands a student's detail view showing 'key messages that contributed to progress', what should be the format? Should it show (a) just the student messages that earned progress, (b) student message + AI response pairs, or (c) student message + a brief AI-generated summary of what was understood?",
        "answer": "**Option (b) — student message + AI response pairs — is the closest match, but the PRD intentionally leaves the exact format flexible.**\n\nThe PRD describes this in two complementary ways:\n\n1. **REQ-DASH-015**: \"The detail view shall show key messages that contributed to goal progress (**task-solution pairs or aggregated message summaries**). These are the messages flagged as `ContributesToProgress` during AI processing.\"\n\n2. **Assignment.md** (original spec): \"key messages that led to progress on the assigned goals. This can also be displayed as **task-solution pairs** or as an **aggregation of multiple messages** that collectively solve a task.\"\n\n3. **Appendix AI Integration Notes #6**: \"Generate Summaries: Create **task-solution pairs** for teacher review.\"\n\nThe word \"or\" in REQ-DASH-015 signals these are two acceptable presentation strategies, not a strict requirement for one:\n\n- **Task-solution pairs** = the student message(s) that earned progress paired with the AI's response acknowledging/confirming that progress (option b).\n- **Aggregated message summaries** = when multiple student messages collectively solved a task, a condensed summary rather than showing every message (closer to option c).\n\n**Recommended implementation**: Use the `ContributesToProgress` flag to filter messages. For each flagged student message, show the student message paired with the immediately following AI response (task-solution pair). This is the simplest approach that satisfies the requirement. The \"aggregated\" format is offered as an alternative for cases where many small messages contribute to a single step — but for the prototype, simply showing the flagged messages with their AI responses is sufficient.\n\nFull chat history is available separately via REQ-DASH-016 / AC-DASH-04, so the detail view's \"key messages\" section is meant to be a curated, digestible summary — not a dump of every message."
      },
      {
        "question": "Should the green border highlighting for progress-contributing messages (REQ-CHAT-007) be applied immediately when the AI response is received, or should there be any delay/animation to draw student attention to the achievement?",
        "answer": "**This is an implementation detail the PRD does not specify — use your best judgment, leaning toward a brief CSS transition rather than an abrupt change.**\n\nThe PRD says:\n- **REQ-CHAT-007**: \"visually highlighted (e.g., green border)\" — the \"e.g.\" signals the green border is a suggestion, not a rigid spec.\n- **AC-CHAT-06 / AC-GOAL-01**: \"that message displays with a green border\" — the acceptance criteria confirm the *end state* (green border visible) but say nothing about transition behavior.\n\nThe PRD does not prescribe immediate vs. animated application. However, the broader theme direction is relevant:\n- The TherapistTemplate theme already uses glow-pulse animations (REQ-THEME-009), glow border effects (REQ-THEME-008), and smooth transitions throughout.\n- An abrupt, unstyled border appearing out of nowhere would feel inconsistent with the rest of the UI.\n\n**Recommended approach**: Apply the green border immediately when the AI response arrives (no artificial delay), but use a short CSS transition (~300-500ms fade-in or glow-in) so it feels natural within the theme. This is not a new feature — it's just standard CSS transition behavior consistent with how the rest of the themed UI already works. Don't overthink this; a simple `transition: border-color 0.3s ease, box-shadow 0.3s ease` is sufficient. No need for a multi-second celebration animation or deliberate delay."
      },
      {
        "question": "For the goal interpretation confirmation flow (REQ-GROUP-005), should the teacher see a preview of how progress increments will work (e.g., 'Step 1 complete = 33%, Step 2 = 66%, Step 3 = 100%'), or just the step count/names?",
        "answer": "**Show the step count AND what constitutes each step — but percentage previews are not required.**\n\nREQ-GROUP-008 specifies exactly what the confirmation screen shall display:\n\n1. **The interpreted goal type** (binary or percentage)\n2. **For percentage goals: the number of steps and what constitutes each step** — e.g., \"3 steps: (1) Solve equation x²+5x+6=0, (2) Solve equation 2x²-3x+1=0, (3) Solve equation x²-4=0\"\n3. **A sample welcome message** the AI would show students\n4. **Accept/Reject buttons**\n\nThe PRD says \"the number of steps and what constitutes each step\" — this means step names/descriptions are required, not just a bare count. However, it does **not** ask for an explicit percentage breakdown preview (33% → 66% → 100%). That said, the percentages are trivially derivable from the step count (it's just 100/N per step), so there's no harm in showing them — but it's not a requirement.\n\n**Recommended implementation**: Show a numbered list of the steps with their descriptions. Optionally show the percentage each step is worth (it's simple math and makes the confirmation more informative), but the PRD only mandates showing what each step *is*, not its percentage value."
      },
      {
        "question": "What should happen if a student's device runs out of battery or loses connection during a session, reconnects hours later (after school ends), and sends a message? Should their session still be fully active, or should there be any session expiration?",
        "answer": "**Yes, the session remains fully active. There is no session expiration — this is intentional for the prototype.**\n\nThe PRD is consistent on this across multiple requirements:\n\n- **REQ-GROUP-025/026**: Groups are permanent and remain accessible indefinitely.\n- **REQ-GROUP-018**: \"If a device attempts to rejoin the same group, the system shall restore the previous session.\"\n- **REQ-GROUP-024**: \"Orphaned sessions remain visible on the teacher dashboard indefinitely. No cleanup mechanism is required for the prototype.\"\n- **REQ-RT-003**: \"Upon reconnection, chat history shall be restored from the server.\"\n- **Out of Scope #27**: \"Orphaned session cleanup — Sessions remain indefinitely.\"\n\nThere is no mention of session TTL, expiration, or any time-based session invalidation anywhere in the PRD, assignment, or constraints. A student reconnecting hours or days later would:\n\n1. Have their session restored (via localStorage device-binding)\n2. See their full chat history\n3. Be able to continue sending messages (unless their goal is already completed, per REQ-GOAL-014)\n4. Trigger a new inactivity alert cycle as normal\n\nThis is explicitly acceptable for the prototype. Session expiration would be a feature to consider in a production version, but it is not in scope. **Implement with no session TTL.**"
      },
      {
        "question": "For the off-topic warning indicator on student messages (REQ-CHAT-008), should it show any text explaining why the message was flagged, or just a visual indicator (icon/color)?",
        "answer": "Use a visual indicator with a brief inline text explanation. The PRD says \"warning indicator...displayed inline with or under the off-topic message\" (REQ-AI-009), suggesting more than just an icon. Since the goal is for students to understand and return to on-topic behavior (which resets the warning per REQ-AI-010), a brief explanatory text helps them learn.\n\n**Recommended implementation (simple approach):**\n- Visual: Orange/amber border or subtle background tint on the message (fits the theme's glow aesthetic)\n- Text: Short inline message below, e.g., \"This seems off-topic. Try to stay focused on your goal.\" \n\nKeep it minimal—one sentence max. No need for detailed explanations or elaborate UI. The visual indicator draws attention, and the text provides enough context without being preachy."
      }
    ],
    "source": "agent"
  }
]